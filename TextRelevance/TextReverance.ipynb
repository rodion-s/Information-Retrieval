{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import functools\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20170702/doc.0000.dat', '20170702/doc.0001.dat']\n"
     ]
    }
   ],
   "source": [
    "docnames = []\n",
    "\n",
    "for d in os.listdir('./content/content'):\n",
    "    listdocs = os.listdir('./content/content' + '/' + d)\n",
    "    listdocs = [d + '/' + doc_name for doc_name in listdocs]\n",
    "    docnames.append(listdocs)\n",
    "docnames = sorted(functools.reduce(operator.iconcat, docnames, []))\n",
    "print(docnames[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv('./urls.numerate.txt', sep='\\t', header=None)\n",
    "urls.index = urls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_url(docname):\n",
    "    with open('./content/content/' + docname, errors='ignore') as read_file:\n",
    "        html = ' '.join(list(read_file))\n",
    "        return html.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d73ba84d7541db919a65c9c660b5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "result = []\n",
    "for doc_name in tqdm(docnames):\n",
    "    url = extract_doc_url(doc_name)\n",
    "    doc_id = urls.at[url, 0]    \n",
    "    result.append((doc_id, doc_name))\n",
    "docnames_ordered_by_doc_id = [v[1] for v in sorted(result, key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20170707/doc.2351.dat',\n",
       " '20170707/doc.2661.dat',\n",
       " '20170707/doc.1883.dat',\n",
       " '20170707/doc.0713.dat',\n",
       " '20170707/doc.0996.dat',\n",
       " '20170707/doc.0995.dat',\n",
       " '20170707/doc.2381.dat',\n",
       " '20170707/doc.3112.dat',\n",
       " '20170707/doc.1037.dat',\n",
       " '20170707/doc.3958.dat',\n",
       " '20170707/doc.1590.dat',\n",
       " '20170707/doc.0645.dat',\n",
       " '20170707/doc.3294.dat',\n",
       " '20170707/doc.4243.dat',\n",
       " '20170707/doc.1350.dat',\n",
       " '20170707/doc.3269.dat',\n",
       " '20170707/doc.0558.dat',\n",
       " '20170707/doc.3762.dat',\n",
       " '20170707/doc.1911.dat',\n",
       " '20170707/doc.1544.dat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docnames_ordered_by_doc_id[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm = pd.read_csv('./sample.technosphere.ir1.textrelevance.submission.txt')\n",
    "tuples = [sample_subm.values[i] for i in range(len(sample_subm))]\n",
    "\n",
    "groups_mapping = dict()\n",
    "for elem in tuples:\n",
    "    if elem[0] not in groups_mapping:\n",
    "        groups_mapping[elem[0]] = [elem[1] - 1]\n",
    "    else:\n",
    "        groups_mapping[elem[0]].append(elem[1] - 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "class GEN_file_to_tokens_title():\n",
    "    def __iter__(self):\n",
    "        for docname in tqdm(docnames_ordered_by_doc_id):\n",
    "            with open('./text-relevance-parsed/content/content/' + docname + '.txt', errors='ignore') as read_file:\n",
    "                title_words = []\n",
    "                for line in read_file:\n",
    "                    if line == '\\n':\n",
    "                        break\n",
    "                    title_words.append(line.rstrip('\\n'))\n",
    "            yield title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "class GEN_file_to_tokens_body():\n",
    "    def __iter__(self):\n",
    "        for docname in tqdm(docnames_ordered_by_doc_id):\n",
    "            with open('./text-relevance-parsed/content/content/' + docname + '.txt', errors='ignore') as read_file:\n",
    "                body_words = []\n",
    "                passed_title = False\n",
    "                for line in read_file:\n",
    "                    if not passed_title:\n",
    "                        if line == '\\n':\n",
    "                            passed_title = True\n",
    "                    else:\n",
    "                        body_words.append(line.rstrip('\\n'))\n",
    "            yield body_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f602c8c4f0b04a89942a8d0e8b02c0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "gen_title_corpus = GEN_file_to_tokens_title()\n",
    "sparse_all_docs_title = vectorizer1.fit_transform(gen_title_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157263ee56d4c60adf2163497e9c41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "gen_body_corpus = GEN_file_to_tokens_body()\n",
    "sparse_all_docs_body = vectorizer2.fit_transform(gen_body_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38114x34400 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 255180 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_all_docs_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38114x3644258 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 78445533 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_all_docs_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_index_in_sparse_title = vectorizer1.vocabulary_\n",
    "term_index_in_sparse_body = vectorizer2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://raw.githubusercontent.com/arosh/BM25Transformer/master/bm25.py')\n",
    "if r.status_code == 200:\n",
    "    with open('Bm25.py', 'w+') as f:\n",
    "        f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodion/Рабочий стол/infopoisk/test_script/Bm25.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "/home/rodion/Рабочий стол/infopoisk/test_script/Bm25.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from Bm25 import BM25Transformer\n",
    "\n",
    "vectorizer3 = BM25Transformer()\n",
    "bm25_vectorized_title = vectorizer3.fit_transform(sparse_all_docs_title)\n",
    "\n",
    "vectorizer4 = BM25Transformer()\n",
    "bm25_vectorized_body = vectorizer4.fit_transform(sparse_all_docs_body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_csv('./queries.numerate.txt', sep='\\t', header=None)\n",
    "queries = queries.values[:, 1]\n",
    "\n",
    "json_path = 'https://speller.yandex.net/services/spellservice.json/checkText?text='\n",
    "\n",
    "corrected_queries = []\n",
    "for query in queries:\n",
    "    r = requests.get(json_path + query)\n",
    "    if r.status_code != 200:\n",
    "        print('ERROR')\n",
    "    changes = {change['word']: change['s'][0] for change in r.json() if len(change['s']) > 0}\n",
    "\n",
    "    corr_query = query\n",
    "    for word, suggestion in changes.items():\n",
    "        corr_query = corr_query.replace(word, suggestion)\n",
    "    \n",
    "    corrected_queries.append(corr_query)\n",
    "queries = corrected_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['же', 'какая', 'во', 'вот', 'себя']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stopwords = list(set(stopwords.words('russian')) - set(['как', 'когда', 'почему', 'зачем', 'чтобы', 'что']))\n",
    "my_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYMORPHY_CACHE = {}\n",
    "def lemmatizer(words):\n",
    "    global PYMORPHY_CACHE\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        if word_hash not in PYMORPHY_CACHE:\n",
    "            PYMORPHY_CACHE[word_hash] = morph.parse(word)[0].normal_form\n",
    "        yield PYMORPHY_CACHE[word_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    query_tokens = list(lemmatizer(query.split()))\n",
    "    query_tokens = [q_tok for q_tok in query_tokens if q_tok not in my_stopwords\n",
    "                    and q_tok not in stopwords.words('english')]\n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ранжирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(vectorized, term_index_in_sparse, query_tokens):\n",
    "    indexes_in_sparse = sorted([term_index_in_sparse[q_tok] for q_tok in query_tokens \n",
    "                                     if q_tok in term_index_in_sparse])\n",
    "    relevant_docs = vectorized[:, indexes_in_sparse].toarray().sum(axis=1)\n",
    "    \n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399it [00:04, 88.07it/s]\n"
     ]
    }
   ],
   "source": [
    "DocsId = []\n",
    "QueryId = []\n",
    "\n",
    "weight = 1.5 # Вес заголовка\n",
    "\n",
    "from tqdm import tqdm\n",
    "for q_id, query in tqdm(enumerate(queries)):\n",
    "    query_tokens = clean_query(query)\n",
    "\n",
    "    candidates = ranking(bm25_vectorized_title[groups_mapping[q_id + 1]], term_index_in_sparse_title, query_tokens) * weight +\\\n",
    "        ranking(bm25_vectorized_body[groups_mapping[q_id + 1]], term_index_in_sparse_body, query_tokens)\n",
    "    \n",
    "    most_relevant_docs_idx = candidates.argsort()[::-1][:10]\n",
    "    \n",
    "    most_relevant_docs = np.array(groups_mapping[q_id + 1])[most_relevant_docs_idx]\n",
    "    for doc_id in most_relevant_docs:\n",
    "        # Запросы в queries.txt нумеруются с единицы\n",
    "        QueryId.append(q_id + 1)\n",
    "        # Документы тоже нумеруются с единицы\n",
    "        DocsId.append(doc_id + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryId</th>\n",
       "      <th>DocumentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>399</td>\n",
       "      <td>38029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>399</td>\n",
       "      <td>38067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>399</td>\n",
       "      <td>38019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>399</td>\n",
       "      <td>38043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>399</td>\n",
       "      <td>38102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3990 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      QueryId  DocumentId\n",
       "0           1          78\n",
       "1           1          28\n",
       "2           1          51\n",
       "3           1          44\n",
       "4           1           7\n",
       "...       ...         ...\n",
       "3985      399       38029\n",
       "3986      399       38067\n",
       "3987      399       38019\n",
       "3988      399       38043\n",
       "3989      399       38102\n",
       "\n",
       "[3990 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'QueryId' : QueryId,\n",
    "    'DocumentId' : DocsId\n",
    "})\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('subm_final.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
